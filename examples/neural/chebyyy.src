

	 --    4-BIT ENCODER LEARNING, BACKPROPAGATION
	 -----------------------------------------------------------
	 display N1 |  display C8 |  display Q |  --        display
	 ------------------------------------------------------------------
	 m=4 |  --                     number of hidden-layer neurons
	 n=16 |  --                          number of network inputs
	 N=n |  --                                number of patterns
	 G=0.95 |  --                                target amplitude
	 --
	 ARRAY x1[n]+x0[1]=layer1,v1[m]+v0[1]=layer2,layer3[n]
	 ARRAY LAYER1[n+1],LAYER2[m+1],LAYER3[n]
	 ARRAY w2[m+1,n+1],w3[n,m+1],DW2[m,n+1],DW3[n,m+1]
	 ARRAY Dw2[m+1,n+1],Dw3[n,m+1]
	 ARRAY INPUT[N,n],target[n]
	 ARRAY error[n],d2[m]+d20[1]=delta2,delta3[n]
	 --
	 x0[1]=1 |  v0[1]=1 |  --                         bias inputs
	 --                                              fill arrays
	 for i=1 to n
	   for k=1 to n
	     INPUT[i,k]=-G
	     next
	   INPUT[i,i]=G
	   next
	 --         small random initial weights to "break symmetry"
	 --
	 for i=1 to m |  for k=1 to n+1
	     w2[i,k]=ran() |  next  |  next
	 for i=1 to n |  for k=1 to m+1
	     w3[i,k]=ran() |  next  |  next
	 -----------------------------------------------------------
	 --                                set experiment parameters
	 gain2=.01 |  gain3=.05
	 mom2=0.09 |  mom3=0.09
	 scale=2 |  cc=8 |  CC=2 |  NN=23000
	 --
	 -----------------------------------------------------------
	 drun  |  STOP  |  --      make a simulation run (NN trials)
	 -----------------------------------------------------------
	    label eee
	 edit 230-270,530-550,820-860
	 STOP  |  --             displays editing screen on shift-F9
	 -----------------------------------------------------------
	    label NOTE
	 note #230-270,530-550,820-860
	 STOP  |  --                   records lines in journal file
	 -----------------------------------------------------------
	 --             test routine, used on command after learning
	    label jjj
	 NNold=NN |  NN=N |  TMAX=NN-1 |  t=1
	 drun RECALL |  --           makes a recall run on shift-F10
	 NN=NNold |  TMAX=NN-1 |  --        restore NN for more runs
	 -----------------------------------------------------------
	 DYNAMIC
	 -----------------------------------------------------------
	 --                                      weight-learning run
	 iRow=t |  Vector x1=INPUT#
	 Vector target=x1
	 Vector layer2=tanh(w2*layer1)
	 Vector layer3=tanh(w3*layer2)
	 ------------------------------------------------------------
	 Vector error=x1-layer3 |  --                backpropagation
	 Vector delta3=error*(1-layer3^2)
	 Vector delta2=w3%*delta3*(1-layer2^2)
	 --                                          "moment" method
	 MATRIX Dw2=gain2*delta2*layer1+mom2*Dw2
	 MATRIX Dw3=gain3*delta3*layer2+mom3*Dw3
	 DELTA w2=Dw2 |  DELTA w3=Dw3
	 ----------------------------------------------------------
	 DOT enormsqr=error*error |  --          accumulate enormsqr
	 MSQERR=enormsqr-scale |  --            offset display curve
	 dispt MSQERR |  --                    display error measure
	 --
	 --                            SHOW display of layers
	 if t-0.996*TMAX
	   Vector LAYER1=cc*swtch(layer1)+CC
	   Vector LAYER3=cc*swtch(layer3)+CC
	   SHOW  |  SHOW LAYER1 |  SHOW LAYER2 |  SHOW LAYER3
	   -----------------------------------------------------------
	   --                                   test run (no learning)
	      label RECALL
	   iRow=t
	   Vector x1=INPUT#
	   Vector layer2=tanh(w2*layer1)
	   Vector layer3=tanh(w3*layer2)
	   -----------------------------
	   Vector error=x1-layer3
	   DOT enormsqr=error*error
	   --
	   z1=layer2[1] |  z2=layer2[2] |  z3=layer2[3]
	   type z1,z2,z3,enormsqr

