

	 --    FUNCTION-LEARNING BACKPROPAGATION NETWORK
	 ---------
	 ---------------------------------------------------------
	 display N1 |  display C8
	 --   define generic 3-layer backpropagation network
	 --
	 --       NOTE: definition does not depend on nx, n2, n3
	 --
	 ARRAY x$[1],y$[1],v$[1],delta2$[1],target$[1],error$[1]
	 ARRAY W1$[1,1],W2$[1,1]
	 SUBMODEL BP(x$,y$,error$,target$,v$,delta2$,W1$,W2$,lrt1$,lrt2$)
	   Vector v$=tanh(W1$*x$) |  --  bias can be included in W2$
	   Vector y$=W2$*v$
	   ----------------------------------------
	   Vector error$=target$-y$
	   Vector delta2$=W2$%*error$*(1-v$^2)
	   DELTA W2$=lrt2$*error$*v$
	   DELTA W1$=lrt1$*delta2$*x$
	   end
	 ------------------------------------------------------------------
	 nx=1 |  ny=1
	 nv=5 |  --                       number of hidden neurons
	 --                         
	 ARRAY x1[nx]+x0[1]=x |  x0[1]=1 |  --       unity bias term
	 ARRAY v[nv],y[ny],target[ny],error[ny],delta2[nv]
	 ARRAY W1[nv,nx+1],W2[ny,nv],Dw1[nv,nx+1],Dw2[ny,nv]
	 --
	 --                         random initial weights "break symmetry"
	 for i=1 to nv
	   W1[i,1]=0.2*ran()
	   W1[i,2]=0.2*ran() |  --                 initial random bias
	   W2[1,i]=0.2*ran()
	   next
	 ------------------------------------------------------------------
	 --                                  set experiment parameters
	 lrate1=0.6 |  lrate2=0.3
	 scale=0.5 |  NN=200000
	 drun  |  display F |  --  training run
	 lrate1=0.3 |  lrate2=0.05 |  NN=10000
	 drun
	 ------------------------------------------------------------------
	 DYNAMIC
	 ------------------------------------------------------------------
	 x1[1]=ran() |  target[1]=0.4*sin(4*x[1]) |  -- input, target
	 --
	 invoke BP(x,y,error,target,v,delta2,W1,W2,lrate1,lrate2)
	 --
	 ERRx100=100*abs(error[1])-scale |  Xin=scale*x[1] |  --  display
	 dispxy Xin,target[1],y[1],ERRx100

