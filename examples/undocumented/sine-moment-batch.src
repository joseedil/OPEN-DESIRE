

	 --       2-LAYER BACKPROPAGATION NETWORK
	 ------------------------------------------------------------------
	 display R |  display N1 |  display C8 |  scale=0.004
	 --
	 nx=1 |  ny=1
	 nv=7 |  --                       number of hidden neurons
	 --                         
	 ARRAY x[nx]+x0[1]=xx |  x0[1]=1 |  --       unity bias term
	 ARRAY v[nv],y[ny],target[ny],error[ny],deltav[nv]
	 ARRAY WW1[nv,nx+1],W2[ny,nv],Dww1[nv,nx+1],Dw2[ny,nv]
	 --
	 ARRAY deltay[ny]
	 --                         random initial weights "break symmetry"
	 for i=1 to nv
	   WW1[i,1]=0.2*ran() |  WW1[i,2]=0.2*ran() |  W2[1,i]=0.2*ran()
	   next
	 ------------------------------------------------------------------
	 --                                   set experiment parameters
	 lrate1=1 |  lrate2=0.1
	 mom1=0.2 |  mom2=0.2
	 NN=100000 |  m=500
	 ----
	 drun  |  --  training run
	 ------------------------------------------------------------------
	 DYNAMIC
	 ------------------------------------------------------------------
	 x[1]=1.1*ran() |  target[1]=0.4*sin(3*x[1]) |  -- input, target
	 --
	 Vector v=tanh(WW1*xx)
	 Vector y=W2*v
	 -------
	 Vector error=target-y
	 Vector deltay=error
	 Vectr delta deltav=W2%*deltay*(1-v^2)
	 Vectr delta deltay=error
	 DOT enormsqr=error*error
	 --
	 SAMPLE m
	 MATRIX Dww1=lrate1*deltav*xx+mom1*Dww1
	 MATRIX Dw2=lrate2*error*v+mom2*Dw2
	 DELTA WW1=Dww1 |  DELTA W2=Dw2
	 Vector deltay=0 |  Vector deltav=0
	 --
	 msqavg=msqavg+(enormsqr-msqavg)/20000
	 MSQAVG=msqavg-scale |  -- offset display                    
	 dispt enormsqr,MSQAVG

