

	 --     RECURRENT-NETWORK PREDICTOR
	 --     predicts Lorenz-attractor time series
	 ---------------------------------------------------------------
	 display N3 |  display C8 |  display R |  scale=1.5
	 TMAX=25 |  DT=0.0001 |  NN=TMAX/DT+1
	 --------------------------------------------------------------
	 nv=17 |  --      number of hidden-layer neurons
	 --
	 --  input, bias, hidden layer (context layer)
	 ARRAY x[3]+x0[1]+y[3]=xx |  x0[1]=1
	 ARRAY v[nv],deltav[nv]
	 ARRAY error[3],target[3]
	 ARRAY WW1[nv,7],W2[3,nv]
	 --
	 for i=1 to nv |  for k=1 to 7 |  -- initialize
	     WW1[i,k]=0.1*ran()
	     next  |  next
	 -----------------------------------------------------------
	 STATE q[3] |  --   for Lorenz state equations
	 --
	 --                        Lorenz-system parameters
	 b=28 |  c=2.6667 |  A=10
	 -----------------------------------------------------------  
	 lrate1=0.0016 |  lrate2=0.0007
	 N=5 |  --                   number of training runs
	 for i=1 to N |  drun  |  next
	 ---------
	 write 'type go for a recall test' |  STOP
	 lrate1=0 |  lrate2=0 |  drun recall |  -- test run
	 -----------------------------------------------------------
	 DYNAMIC
	 -----------------------------------------------------------
	 d/dt q[1]=A*(q[2]-q[1])+0.001 |  -- Lorenz
	 d/dt q[2]=q[1]*(b-q[3])-q[2]
	 d/dt q[3]=q[1]*q[2]-c*q[3]
	 ---------------------------------------------------------------
	 OUT  |  --   we use sampled data from now on!
	 Vector target=0.05*q
	 Vector x=target
	 Vector v=tanh(WW1*xx) |  --    includes bias
	 Vector y=W2*v |  --  no output limiter needed
	 --
	 Vector error=target-y
	 Vector deltav=W2%*error*(1-v^2) |  -- backprop
	 DELTA WW1=lrate1*deltav*xx
	 DELTA W2=lrate2*error*v
	 dispxy x[1],x[2]
	 --------------------------------------------------------------
	    label recall
	 x[1]=0.001
	 Vector v=tanh(WW1*xx) |  --    includes bias
	 Vector y=W2*v |  --  no output limiter needed
	 dispxy x[1],x[2]

