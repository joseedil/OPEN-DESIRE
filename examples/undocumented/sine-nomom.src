

	 --  2-LAYER BACKPROPAGATION NETWORK
	 ------------------------------------------------------------------
	 display Q |  display N1 |  display C8 |  scale=0.5
	 --
	 nx=1 |  ny=1
	 nv=7 |  --                       number of hidden neurons
	 --                         
	 ARRAY x[nx]+x0[1]=xx |  x0[1]=1 |  --       unity bias term
	 ARRAY v[nv],y[ny],target[ny],error[ny],deltav[nv]
	 ARRAY WW1[nv,nx+1],W2[ny,nv]
	 --
	 for i=1 to nv |  -- initialize
	   WW1[i,1]=0.2*ran() |  WW1[i,2]=0.2*ran() |  W2[1,i]=0.2*ran()
	   next
	 ------------------------------------------------------------------
	 --                                   set experiment parameters
	 lrate1=0.7 |  lrate2=0.1
	 NN=100000
	 ----
	 for i=1 to 10 |  drun  |  next  |  --  training runs
	 write "type go for a test run" |  STOP
	 display R
	 lrate1=0 |  lrate2=0 |  NN=8000 |  -- test run
	 drun
	 ------------------------------------------------------------------
	 DYNAMIC
	 ------------------------------------------------------------------
	 x[1]=1.1*ran() |  target[1]=0.4*sin(3*x[1]) |  -- input, target
	 --
	 Vector v=tanh(WW1*xx)
	 Vector y=W2*v
	 ----------------------------------------
	 Vector error=target-y
	 Vector deltav=W2%*error*(1-v^2)
	 DELTA WW1=lrate1*deltav*xx
	 DELTA W2=lrate2*error*v
	 --                                                                
	 ABSERRx100=100*abs(error[1])-scale |  Xin=scale*x[1]
	 dispxy Xin,target[1],y[1],ABSERRx100

